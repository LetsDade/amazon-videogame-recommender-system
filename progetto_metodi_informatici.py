# -*- coding: utf-8 -*-
"""Progetto_metodi_informatici

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rfXJCgldwIiFXS-R87njN197_Qaa4J5A
"""

# Frosi Davide
# Progetto Metodi Informatici per la Gestione Aziendale
# Struttura: Progetto Base, Intermedio, Avanzato

!pip install -U datasets==2.16.0
from datasets import load_dataset
dataset = load_dataset('McAuley-Lab/Amazon-Reviews-2023', 'raw_review_Video_Games')
Review = dataset['full'].to_pandas()
Review_Video_Games = Review[['rating', 'parent_asin', 'user_id','title','text']] # Seleziono solo le informazioni essenziali, togliendo il resto
df_correlazione = Review[['rating','timestamp','verified_purchase','helpful_vote']] # Prendo queste colonne perchè utili per fare analisi di correlazione

# PROGETTO BASE
# STEP 1
# In questo step devo fare un'analisi esplorativa

# Statistiche descrittive

# Controllo che non ci siano missing values
missing_values = (Review_Video_Games.isnull().sum())
print(missing_values)

# Head e Tail
print(Review_Video_Games.head())
print(Review_Video_Games.tail())
# Statistiche base sul rating
print(Review_Video_Games['rating'].describe())

#Distribuzione dei rating
import matplotlib.pyplot as plt
import seaborn as sns

rating_counts = Review_Video_Games['rating'].value_counts()
print(rating_counts) # Numero di valutazioni per ognuno dei 5 livelli di Rating

plt.figure(figsize=(8,5))
sns.countplot(data=Review_Video_Games, x='rating', palette='viridis')
plt.title('Distribuzione dei rating')
plt.xlabel('Rating')
plt.ylabel('Numero di recensioni')
plt.show()

# Distribuzione degli utenti in base al numero di recensioni
user_review_counts = Review_Video_Games['user_id'].value_counts()
print(user_review_counts) # Mi serve per capire come suddividere le recensioni in fasce, intuisco quanti utenti hanno un determinato numero di recensioni
u_review_counts = user_review_counts.values

plt.figure(figsize=(15, 10))
fasce = ['1', '2', '3-5','6+'] # Suddivido in fasce le recensioni
conteggi = []
conteggi.append(sum(1 for x in u_review_counts if x == 1))
conteggi.append(sum(1 for x in u_review_counts if x == 2))
conteggi.append(sum(1 for x in u_review_counts if 3 <= x <= 5))
conteggi.append(sum(1 for x in u_review_counts if x > 6))

plt.bar(fasce, conteggi, alpha=0.7, color='lightgreen', edgecolor='black')
plt.title('Distribuzione degli utenti in base al numero di recensioni')
plt.xlabel('Fasce numero recensioni')
plt.ylabel('Numero di utenti')
plt.show()

import matplotlib.pyplot as plt
# Distribuzione dei prodotti in base al numero di recensioni
item_review_counts = Review_Video_Games['parent_asin'].value_counts()
print(item_review_counts)
i_review_counts = item_review_counts.values
plt.figure(figsize=(15, 10))
# Suddivido in fasce le recensioni, ho usato un criterio logico: la fiducia che l'utente può riporre nelle recensioni (1-10:dati scarsi,11-100:prodotto più consolidato, 101+:prodotto molto popolare)
fasce = ['1-10', '11-100', '101+']
conteggi = []

conteggi.append(sum(1 for x in i_review_counts if 1 <= x <= 10))
conteggi.append(sum(1 for x in i_review_counts if 11 <= x <= 100))
conteggi.append(sum(1 for x in i_review_counts if x > 100))

plt.bar(fasce, conteggi, alpha=0.7, color='orange', edgecolor='black')
plt.title('Distribuzione dei prodotti in base al numero di recensioni')
plt.xlabel('Fasce numero recensioni')
plt.ylabel('Numero di prodotti')
plt.show()

# Analisi correlazione
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# Correlazione tra rating e helpful_vote
# Scatterplot + coefficiente di correlazione
print(df_correlazione[['rating', 'helpful_vote']].corr())

sns.scatterplot(data=df_correlazione, x='rating', y='helpful_vote')
plt.title('Rating vs Helpful Votes')
plt.show()

# Correlazione tra rating e verified_purchase
# Convertiamo 'verified_purchase' in numerico (True/False → 1/0)
df_correlazione['verified_purchase_num'] = df_correlazione['verified_purchase'].astype(int)

print(df_correlazione[['rating', 'verified_purchase_num']].corr())

sns.boxplot(data=df_correlazione, x='verified_purchase', y='rating')
plt.xlabel('Acquisto verificato')
plt.title('Rating per acquisti verificati')
plt.show()

# Correlazione tra rating e timestamp
# Convertiamo timestamp in data leggibile
df_correlazione['date'] = pd.to_datetime(df_correlazione['timestamp'], unit='ms')
df_correlazione['year'] = df_correlazione['date'].dt.year

# Media dei rating per anno
media_anno = df_correlazione.groupby('year')['rating'].mean().reset_index()
print(media_anno)

plt.figure(figsize=(10, 4))
plt.plot(media_anno['year'], media_anno['rating'], marker='o')
plt.title('Variazione del rating medio per anno')
plt.xlabel('Anno')
plt.ylabel('Rating medio')
plt.grid(True)
plt.tight_layout()
plt.show()

# STEP 2
# In questo step devo identificare la configurazione ottimale dell’algoritmo K-NN per la predizione dei rating, tramite le diverse metriche di performance (MSE e RMSE). Testerò le diverse combinazioni: similarità, valore di K, user/item based.
# Devo filtrare le recensioni, sia per ridurre poi la dimensione della matrice sia per aumentare la qualità del sistema di raccomandazione
# Conto recensioni per user
user_counts = Review_Video_Games['user_id'].value_counts()
users_attivi = user_counts[user_counts > 25].index

# Conto recensioni per prodotto
item_counts = Review_Video_Games['parent_asin'].value_counts()
items_popolari = item_counts[item_counts > 25].index
# Ho deciso di filtrare prendendo solo utenti e prodotti con più di 25 recensioni
# Applico il filtro
Review_Video_Games_filtrato = Review_Video_Games[Review_Video_Games['user_id'].isin(users_attivi) & Review_Video_Games['parent_asin'].isin(items_popolari)]

# Guardo la dimensione prima del filtro
print(f"Numero righe prima del filtraggio: {Review_Video_Games.shape[0]}")
print(f"Utenti unici: {Review_Video_Games['user_id'].nunique()}")
print(f"Prodotti unici: {Review_Video_Games['parent_asin'].nunique()}")
# Guardo la dimensione dopo il filtro
print(f"Numero righe dopo il filtraggio: {Review_Video_Games_filtrato.shape[0]}")
print(f"Utenti unici: {Review_Video_Games_filtrato['user_id'].nunique()}")
print(f"Prodotti unici: {Review_Video_Games_filtrato['parent_asin'].nunique()}")

from surprise import Dataset, Reader
from surprise.model_selection import train_test_split
from surprise import accuracy

reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(Review_Video_Games_filtrato[['user_id', 'parent_asin', 'rating']], reader)

trainset, testset = train_test_split(data, test_size=0.2, random_state=1)
from surprise import KNNBasic

def v_knn(similarity_type, user_based, k):
    sim_options = {'name': similarity_type,'user_based': user_based}

    algo = KNNBasic(k=k, sim_options=sim_options)
    algo.fit(trainset)
    predictions = algo.test(testset)

    mse = accuracy.mse(predictions, verbose=False)
    rmse = accuracy.rmse(predictions, verbose=False)

    return {
        'Similarity': similarity_type,
        'User-Based': user_based,
        'K': k,
        'MSE': mse,
        'RMSE': rmse
    }
risultati = []

for sim in ['cosine', 'pearson']:
    for user_based in [True, False]:  # True: user-user (User-based), False: item-item (Item-based)
        for k in [10, 20, 40]:        # Ho cercato di prendere valori di k che non fossero nè troppo grandi nè troppo piccoli
            res = v_knn(sim, user_based, k)
            print(f"Valutata combinazione: {res}")
            risultati.append(res)

# Ho usato come similarità sia la similarità coseno che Pearson

# Converto in DataFrame per visualizzare il risultato finale
import pandas as pd
risultati_df = pd.DataFrame(risultati)
risultati_df = risultati_df.sort_values('RMSE') # Metto in ordine crescente, basandomi sui risultati del Root Mean Squared Error
risultati_df
# La configurazione ottimale che ottengo è: K = 40, Similarity = similarità coseno, Item-based

# STEP 3
# In questo step devo fare filling della matrice di rating con la configurazione ottimale appena trovata
from surprise import Dataset, Reader
from surprise.model_selection import train_test_split
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(Review_Video_Games_filtrato[['user_id', 'parent_asin', 'rating']], reader)

trainset, testset = train_test_split(data, test_size=0.2, random_state=1)

from surprise import KNNBasic
# Uso la configurazione ottimale
configurazione_migliore = {'name': 'cosine','user_based': False}
k_migliore = 40

algo = KNNBasic(k=K_migliore, sim_options=configurazione_migliore)
algo.fit(trainset)

from collections import defaultdict

# Prendo tutti gli user / item mancanti nel training set
trainset_users = set([trainset.to_raw_uid(u) for u in trainset.all_users()])
trainset_items = set([trainset.to_raw_iid(i) for i in trainset.all_items()])

# Ora genero tutte le possibili coppie user-item
coppie_possibili = []
for uid in trainset_users:
    for iid in trainset_items:
        if not trainset.knows_user(trainset.to_inner_uid(uid)) or not trainset.knows_item(trainset.to_inner_iid(iid)):
            continue
        if not trainset.ur[trainset.to_inner_uid(uid)] or iid in [trainset.to_raw_iid(i[0]) for i in trainset.ur[trainset.to_inner_uid(uid)]]:
            continue
        coppie_possibili.append((uid, iid))
print(f"Numero di rating da predire: {len(coppie_possibili)}")

# Dopo aver calcolato quanti sono i rating da predire procedo a realizzare effettivamente il filling
from surprise import PredictionImpossible

ratings_fillati = []

for uid in trainset_users:
    for iid in trainset_items:
        try:
            pred = algo.predict(uid, iid)
            ratings_fillati.append((uid, iid, pred.est))
        except PredictionImpossible:
            continue

# Ora posso printare, per esempio, dieci predizioni (random) fatte per un utente
for tupla_ratings in ratings_fillati[:10]:
    print(tupla_ratings)

# STEP 4
# In questo step devo segmentare gli utenti in base alle loro preferenze applicando l'algoritmo di clustering K-Means con similarità coseno
# Costruisco una matrice user-item (con i rating reali, non predetti)
pivot_df = Review_Video_Games_filtrato.pivot_table(index='user_id',columns='parent_asin',values='rating').fillna(0)
# Faccio una scelta pratica: nei rating mancanti metto come valore 0

from sklearn.preprocessing import normalize
# Normalizzo
pivot_normalizzata = normalize(pivot_df, norm='l2', axis=1)

from sklearn.cluster import KMeans
wcss = []
for i in range(1, 15):
    km = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=1)
    km.fit(pivot_normalizzata)
    wcss.append(km.inertia_)

import matplotlib.pyplot as plt
# Elbow method per trovare il k ottimale
plt.plot(range(1, 15), wcss, marker='o')
plt.title('The Elbow Method')
plt.xlabel('Numero di Cluster')
plt.ylabel('WCSS')
plt.grid(True)
plt.show()

# Silhouette score
from sklearn.metrics import silhouette_score

for k in range(2, 15):
    km = KMeans(n_clusters=k, random_state=1)
    labels = km.fit_predict(pivot_normalizzata)
    score = silhouette_score(pivot_normalizzata, labels)
    print(f"K={k} → silhouette score: {score:.4f}")

# Guardando il grafico dell'elbow method e il silhouette score, il valore ottimale che scelgo è k = 8

# Applico il clustering con K=8
kmeans = KMeans(n_clusters=8, init = 'k-means++', max_iter = 300, n_init = 10, random_state=1)
cluster_labels = kmeans.fit_predict(pivot_normalizzata)

# Aggiungo i cluster alla matrice originale
pivot_df['cluster'] = cluster_labels
pivot_df.reset_index(inplace=True)
pivot_df[['user_id', 'cluster']].head()

# STEP 5
# Ora procedo con la creazione per ogni utente della lista degli n items (top k items) da consigliare
!pip install surprise

from surprise import Dataset, Reader
from surprise.model_selection import train_test_split

reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(Review_Video_Games_filtrato[['user_id', 'parent_asin', 'rating']], reader)

trainset, testset = train_test_split(data, test_size=0.2, random_state=1)

# Scelgo un N
TOP_N = 5
# Costruisco una funzione per ottenere i top-N per ogni utente
from collections import defaultdict
from surprise import KNNBasic

# Prima di tutto rifaccio il training del modello migliore
sim_options = {'name': 'cosine','user_based': False}
algo = KNNBasic(k=40, sim_options=sim_options)
algo.fit(trainset)

# Calcolo numero recensioni per ogni item nel dataset filtrato (questo mi servirà per avere un criterio di ordine nell'esposizione dei top N items da raccomandare)
item_review_counts = Review_Video_Games_filtrato['parent_asin'].value_counts().to_dict()

# Calcolo i top-N predetti (mantengo il 5 come il TOP N che ho scelto, ma chiaramente potremmo calcolarne di più al bisogno)
def f_top_n(predictions, n=5, review_counts=None):
    top_n = defaultdict(list)
    for uid, iid, true_r, est, _ in predictions:
        num_reviews = review_counts.get(iid, 0) if review_counts else 0
        top_n[uid].append((iid, est, num_reviews))

# Qui ordino per rating, poi in secondo piano ordinerò anche per il numero di recensioni (a parità di valutazione, preferisco mettere prodotti che hanno il maggior numero di valutazioni)
    for uid, user_ratings in top_n.items():
        top_n[uid] = sorted(user_ratings, key=lambda x: (x[1], x[2]), reverse=True)[:n]
    return top_n

# Costruisco lista completa user-item da predire
trainset_users = set([trainset.to_raw_uid(u) for u in trainset.all_users()])
trainset_items = set([trainset.to_raw_iid(i) for i in trainset.all_items()])

# Per una questione di RAM disponibile e per facilità di confronto prendo solo un campione casuale di 100 utenti
import random
random.seed(1)
campione_users = random.sample(list(trainset_users), 100)

import json
with open("campione_users_100.json", "w") as f:
    json.dump(campione_users, f)

# Creo le coppie (user, item) solo per quelli non ancora valutati
testset = []
for uid in campione_users:
    items_valutati = set([trainset.to_raw_iid(iid) for (iid, _) in trainset.ur[trainset.to_inner_uid(uid)]])
    items_non_valutati = trainset_items - items_valutati
    testset.extend((uid, iid, 0) for iid in items_non_valutati)

# Faccio le predictions
predictions = algo.test(testset)

# Ottengo i top-N (tenendo conto che, a parità di valutazione, ordino per il numero di recensioni)
knn_raccomandazioni = f_top_n(predictions, n=TOP_N, review_counts=item_review_counts)

import pandas as pd

# Costruisco una lista di righe per poi mettere tutto in un dataframe in modo da facilitare la visione dell'output finale
def top_n_in_dataframe(top_n_dict):
    rows = []
    for uid, racc in top_n_dict.items():
        row = {'user_id': uid}
        for i, (iid, rating, reviews) in enumerate(racc, 1):
            row[f'item_{i}'] = iid
            row[f'rating_{i}'] = round(rating, 2)
            row[f'reviews_{i}'] = reviews
        rows.append(row)
    return pd.DataFrame(rows)

# Ad esempio ora posso mostrare i primi 10 utenti e i loro 5 prodotti raccomandati, in ordine di valutazione in primis e in caso di parità, guardando il numero di recensioni
subset_top_n = dict(list(knn_raccomandazioni.items())[:10])
top_n_df = top_n_in_dataframe(subset_top_n)
top_n_df.head(10)

# STEP 6
# In questo step devo utilizzare la Matrix Factorization per predire i rating, per poi confrontare le performance con KNN tramite RMSE e MSE.

from surprise import SVD
from surprise import accuracy

# Prima di tutto, alleno il modello SVD
from surprise import Dataset, Reader
from surprise.model_selection import train_test_split
reader = Reader(rating_scale=(1, 5))
data = Dataset.load_from_df(Review_Video_Games_filtrato[['user_id', 'parent_asin', 'rating']], reader)

trainset, testset = train_test_split(data, test_size=0.2, random_state=1)

svd = SVD()
svd.fit(trainset)

# Predizioni sul test set
predictions_svd = svd.test(testset)

# Calcolo RMSE e MSE per SVD
rmse_svd = accuracy.rmse(predictions_svd)
mse_svd = accuracy.mse(predictions_svd)

# Ora posso confrontare con KNN
from surprise import KNNBasic

# Riprendo la stessa configurazione ottimale dello step 2
sim_options = {'name': 'cosine', 'user_based': False}
knn = KNNBasic(k=40, sim_options=sim_options)
knn.fit(trainset)

predictions_knn = knn.test(testset)

rmse_knn = accuracy.rmse(predictions_knn)
mse_knn = accuracy.mse(predictions_knn)

# Printo il confronto finale
print("\nCONFRONTO FINALE")
print(f"KNN → RMSE: {rmse_knn:.4f} | MSE: {mse_knn:.4f}")
print(f"SVD → RMSE: {rmse_svd:.4f} | MSE: {mse_svd:.4f}")

!pip install -U datasets==2.16.0
from datasets import load_dataset
metadata = load_dataset("McAuley-Lab/Amazon-Reviews-2023", 'raw_meta_Video_Games')
metadata_df = metadata['full'].to_pandas()
metadata_df = metadata_df[['parent_asin', 'title', 'description']]

# PROGETTO INTERMEDIO
# STEP 1
# Innanzitutto applico lo stesso filtro che avevo messo nel progetto base, per gli stessi motivi e in generale mantenere coerenza, essendo che poi dovremo confrontarli
item_filtrati = Review_Video_Games_filtrato['parent_asin'].unique()
# Considero solo gli item rimasti dopo l'applicazione del filtro
metadata_df = metadata_df[metadata_df['parent_asin'].isin(item_filtrati)]

# In questo step devo processare gli attributi testuali dei prodotti (prendo title e description), in modo tale che siano pronti per gli embedding
# Creo una nuova colonna con title + description
metadata_df['text'] = metadata_df['title'].fillna('').astype(str) + ' ' + metadata_df['description'].fillna('').astype(str)
metadata_df['text_clean'] = metadata_df['text'].str.strip()

# Faccio pulizia e preprocessing del testo
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def clean_text(text):
    text = str(text).strip().lower() # Converto tutto in minuscolo
    text = re.sub(r'[^a-z\s]', '', text) # Rimuovo tutto tranne lettere e spazi
    text = re.sub(r'\s+', ' ', text).strip() # Metto un singolo spazio al posto di più spazi consecutivi
    tokens = text.split() # Divido il testo in parole (token)
    tokens = [word for word in tokens if word not in stop_words]  # Rimuovo le stopwords
    tokens = [stemmer.stem(word) for word in tokens] # Faccio la normalizzazione del testo riducendo le parole alla loro forma radice.
    text = ' '.join(tokens)
    text = re.sub(r'\s+', ' ', text).strip() # Metto un singolo spazio al posto di più spazi consecutivi dopo aver rimosso le stop words
    return text

# Applico la funzione per aggiornare la colonna creata con il testo "pulito"
metadata_df['text_clean'] = metadata_df['text'].apply(clean_text)

# Ora posso printare l'head del testo "pulito"
print(metadata_df['text_clean'].head)

# STEP 2
# In questo step creo gli embedding, prima con tecnica basata sulla frequenza (TFIDF), poi con tecnica neurale (transformers)
# TFIDF
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(max_features=5000, min_df=5, max_df=0.95, token_pattern=r'(?u)\b\w\w+\b')
# Considero le prime 5000 parole più frequenti e rilevanti, prendendo solo parole che appaiono almeno in 5 item e escludendo le parole presenti in più del 95% degli item.
# Applico poi il TFIDF sulla colonna pulita
tfidf_matrix = tfidf.fit_transform(metadata_df['text_clean'])

# Verifico la shape
print(f"TFIDF shape: {tfidf_matrix.shape}")

# Ora posso printare l'embedding dei prodotti (prendo un sample di 5 items come esempio)
import pandas as pd
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), index=metadata_df['parent_asin'], columns=tfidf.get_feature_names_out())
tfidf_df.head(5)

# TRANSFORMERS
!pip install transformers --quiet
!pip install sentence-transformers --quiet
from sentence_transformers import SentenceTransformer

# Carico un modello pre-addestrato
model = SentenceTransformer('all-MiniLM-L6-v2')

# Uso il campo già pulito
texts = metadata_df['text_clean'].tolist()

# Embedding di tutti i prodotti
embeddings = model.encode(texts, batch_size=16)

# Verifico la shape
print(f"Transformers shape: {embeddings.shape}")

# Converto tutto in una dataframe
import pandas as pd
embedding_df = pd.DataFrame(embeddings, index=metadata_df['parent_asin'])
embedding_df.head()

# STEP 3
# In questo step devo effettuare predizioni per ogni utente usando KNN come algoritmo, usando gli embedding ottenuti con le due tecniche dello step 2
matrice_ratings = Review_Video_Games_filtrato.pivot_table(index='user_id', columns='parent_asin', values='rating')

from sklearn.metrics.pairwise import cosine_similarity
# TFIDF
# Calcolo la similarità item-item
tfidf_sim_matrix = cosine_similarity(tfidf_matrix)
tfidf_sim_df = pd.DataFrame(tfidf_sim_matrix, index=metadata_df['parent_asin'], columns=metadata_df['parent_asin'])

# Transformers
# Calcolo la similarità item-item
transf_sim_matrix = cosine_similarity(embedding_df.values)
transf_sim_df = pd.DataFrame(transf_sim_matrix, index=embedding_df.index, columns=embedding_df.index)

# Creo una funzione per predire i top_N item per un utente
import numpy as np

def top_n_raccomandazioni(user_id, matrice_ratings, sim_df,item_review_counts, k_neighbors=5, top_n=5):
    items_valutati = matrice_ratings.loc[user_id].dropna()
    items_non_valutati = matrice_ratings.loc[user_id][matrice_ratings.loc[user_id].isna()].index

    predictions = []

    for item in items_non_valutati:
        similarità = sim_df.loc[item, items_valutati.index]
        top_k = similarità.sort_values(ascending=False).head(k_neighbors)
        top_k_ratings = items_valutati[top_k.index]

        if top_k_ratings.empty or top_k.sum() == 0:
            continue

        rating_predetto = np.dot(top_k, top_k_ratings) / top_k.sum()
        rating_predetto = min(5.0, max(1.0, rating_predetto))
        num_reviews = item_review_counts.get(item, 0)
        predictions.append((item, rating_predetto, num_reviews))

# Ordino prima per rating, poi per numero di recensioni
    predictions_ordinate = sorted(predictions, key=lambda x: (x[1], x[2]), reverse=True)
    return [(item, rating) for item, rating, _ in predictions_ordinate[:top_n]]

# Calcolo il numero recensioni per ogni item (questo mi servirà per avere un criterio di ordine nell'esposizione dei top N items da raccomandare, coerente con quanto fatto nel progetto base)
item_review_counts = Review_Video_Games_filtrato['parent_asin'].value_counts().to_dict()

# Per una questione di RAM disponibile e per facilità di confronto prendo solo un campione casuale di 100 utenti (rimanendo coerente con quanto fatto nel progetto base)
import json
with open("campione_users_100.json") as f:
    campione_users_100 = json.load(f)

tfidf_raccomandazioni = {}
transf_raccomandazioni = {}

# TFIDF
for user_id in campione_users_100:
    tfidf_raccomandazioni[user_id] = top_n_raccomandazioni(user_id=user_id, matrice_ratings=matrice_ratings, sim_df=tfidf_sim_df, item_review_counts=item_review_counts, k_neighbors=5, top_n=5)

# Transformers
for user_id in campione_users_100:
    transf_raccomandazioni[user_id] = top_n_raccomandazioni(user_id=user_id, matrice_ratings=matrice_ratings, sim_df=transf_sim_df, item_review_counts=item_review_counts, k_neighbors=5, top_n=5)

# Metto tutto in dataframe
tfidf_df = pd.DataFrame([{"user_id": user, "item_id": item, "titolo": metadata_df.loc[metadata_df['parent_asin'] == item, 'title'].values[0] if not metadata_df.loc[metadata_df['parent_asin'] == item, 'title'].empty else 'Titolo non trovato', "rating predetto": rating, "recensioni": item_review_counts.get(item, 0)}
for user, racc in tfidf_raccomandazioni.items()
for item, rating in racc])

transf_df = pd.DataFrame([{"user_id": user, "item_id": item, "titolo": metadata_df.loc[metadata_df['parent_asin'] == item, 'title'].values[0] if not metadata_df.loc[metadata_df['parent_asin'] == item, 'title'].empty else 'Titolo non trovato', "rating predetto": rating, "recensioni": item_review_counts.get(item, 0)}
for user, racc in transf_raccomandazioni.items()
for item, rating in racc])

print("=== TFIDF ===")
print(tfidf_df.head())

print("\n=== Transformers ===")
print(transf_df.head())

# Analisi più approfondite nei 2 step finali

# Overlap per lo step 4 e 5
overlap_tfidf_transf = []
overlap_cb_cf = []

for user in campione_users_100:
    tfidf_items = [item for item, _ in tfidf_raccomandazioni.get(user, [])]
    transf_items = [item for item, _ in transf_raccomandazioni.get(user, [])]
    cf_items = [item for item, _, _ in knn_raccomandazioni.get(user, [])]

    # TFIDF vs Transf
    overlap_tb = len(set(tfidf_items) & set(transf_items)) / 5
    overlap_tfidf_transf.append(overlap_tb)

    # Content-Based vs CF
    overlap_cbcf = len(set(transf_items) & set(cf_items)) / 5 if cf_items else 0
    overlap_cb_cf.append(overlap_cbcf)

print(f"Media overlap TFIDF/Transf: {np.mean(overlap_tfidf_transf):.2f}")
print(f"Media overlap CB/CF: {np.mean(overlap_cb_cf):.2f}")

# Distribuzione dei rating predetti per lo step 4
import matplotlib.pyplot as plt

tfidf_ratings = [rating for racc in tfidf_raccomandazioni.values() for _, rating in racc]
transf_ratings = [rating for racc in transf_raccomandazioni.values() for _, rating in racc]

plt.hist(tfidf_ratings, bins=20, alpha=0.6, label='TFIDF')
plt.hist(transf_ratings, bins=20, alpha=0.6, label='Transf')
plt.xlabel('Rating predetto')
plt.ylabel('Frequenza')
plt.legend()
plt.title('Distribuzione dei rating predetti')
plt.grid(True)
plt.show()

# Distribuzione della popolarità degli item raccomandati per lo step 5
import pandas as pd
import matplotlib.pyplot as plt

def plot_distribuzione_popolarità(raccomandazioni_diz, nome_modello):
    t_items = []
    for racc in raccomandazioni_diz.values():
        for rac in racc:
            if len(rac) == 3:
                t_items.append(rac[0])
            elif len(rac) == 2:
                t_items.append(rac[0])

    item_counts = pd.Series(t_items).value_counts()

    plt.figure(figsize=(10, 4))
    item_counts.plot(kind='hist', bins=20, edgecolor='black')
    plt.title(f'Distribuzione popolarità item - {nome_modello}')
    plt.xlabel('Numero di volte raccomandato')
    plt.ylabel('Frequenza')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

plot_distribuzione_popolarità(tfidf_raccomandazioni, 'TFIDF')
plot_distribuzione_popolarità(transf_raccomandazioni, 'Transformers')
plot_distribuzione_popolarità(knn_raccomandazioni, 'Collaborative Filtering')

!pip install datasets==2.16.0
from datasets import load_dataset
dataset = load_dataset('McAuley-Lab/Amazon-Reviews-2023', 'raw_review_Video_Games')
Review_Video_Games_A = dataset['full'].to_pandas()
Review_Video_Games_A = Review_Video_Games_A.drop(columns=['user_id', 'images', 'asin', 'timestamp', 'verified_purchase', 'helpful_vote']) # Seleziono solo le informazioni essenziali, togliendo il resto

# PROGETTO AVANZATO
# STEP 1
# In questo step devo processare gli attributi testuali delle review (considero title e text) con tecniche di NLP
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
nltk.download('punkt_tab')

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# Creo una nuova colonna con title + text
Review_Video_Games_A['review_text'] = Review_Video_Games_A['title'].fillna('').astype(str) + ' ' + Review_Video_Games_A['text'].fillna('').astype(str)
Review_Video_Games_A['review_text_clean'] = Review_Video_Games_A['review_text'].str.strip()

# Faccio pulizia e preprocessing del testo
def clean_text(text):
    text = str(text).strip().lower() # Converto tutto in minuscolo
    text = re.sub(r'[^a-zA-Z\s]', '', text) # Tolgo punteggiatura e simboli
    text = re.sub(r'\d+', '', text) # Tolgo i numeri
    text = re.sub(r'\s+', ' ', text).strip() # Elimino spazi extra
    tokens = nltk.word_tokenize(text) # Tokenizzazione
    tokens = [t for t in tokens if t not in stop_words]
    tokens = [lemmatizer.lemmatize(t) for t in tokens] # Lemmatizzazione
    return ' '.join(tokens)

# Applico la funzione per aggiornare la colonna creata con il testo "pulito"
Review_Video_Games_A['review_text_clean'] = Review_Video_Games_A['review_text'].apply(clean_text)

# Ora posso printare le prime review con testo "pulito"
print(Review_Video_Games_A['review_text_clean'].head(5))

# STEP 2
# In questo step creo gli embedding, prima con tecnica basata sulla frequenza (TFIDF), poi con tecnica neurale (transformers)
# TFIDF

# Per questioni di RAM disponibile e rappresentazione, considero solo un campione di tutte le review
campione_review = Review_Video_Games_A.dropna(subset=['review_text_clean']).sample(40000, random_state=1)

from sklearn.feature_extraction.text import TfidfVectorizer
# Considero le prime 5000 parole più frequenti e rilevanti, prendendo solo parole che appaiono almeno in 100 review e escludendo le parole presenti in più del 90% delle review
tfidf_vectorizer = TfidfVectorizer(max_features=5000,min_df=15,max_df=0.95,token_pattern=r'(?u)\b\w\w+\b')

# Applico poi il TFIDF sulla colonna "pulita"
X_tfidf = tfidf_vectorizer.fit_transform(campione_review['review_text_clean'])

# Verifico la shape
print(f"TFIDF shape: {X_tfidf.shape}")

# Converto tutto in una dataframe
import pandas as pd
df_tfidf = pd.DataFrame(X_tfidf.toarray(), index=campione_review['parent_asin'], columns=tfidf_vectorizer.get_feature_names_out())
df_tfidf.head()

# TRANSFORMERS
!pip install -q sentence-transformers
from sentence_transformers import SentenceTransformer

# Carico un modello pre-addestrato
transf_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

# Uso il campo già pulito
review_texts = list(campione_review['review_text_clean'])

# Embeddings del testo delle review
X_transf = transf_model.encode(review_texts, batch_size=16, show_progress_bar=True)

# Verifico la shape
print(f"Transformers shape: {X_transf.shape}")

# Converto tutto in una dataframe
import pandas as pd
df_transf = pd.DataFrame(X_transf, index=campione_review['parent_asin'])
df_transf.head()

# STEP 3
# In questo step devo effettuare la predizione del sentiment (rating 1-2: sentiment negativo, rating 3: sentiment neutro, rating 4-5: sentiment positivo)

# Creo la variabile target (sentiment)
def label_sentiment(rating):
    if rating in [1, 2]:
        return 'neg'
    elif rating == 3:
        return 'neu'
    else:
        return 'pos'

campione_review['sentiment'] = campione_review['rating'].apply(label_sentiment)

# Splitto in train e test

# TFIDF
from sklearn.model_selection import train_test_split

X_tfidf_train, X_tfidf_test, y_train, y_test = train_test_split(df_tfidf.values,campione_review['sentiment'],test_size=0.2,random_state=1)

# TRANSFORMERS
X_transf_train, X_transf_test, _, _ = train_test_split(df_transf.values,campione_review['sentiment'],test_size=0.2,random_state=1)

# Uso 2 classificatori: LogisticRegression e RandomForest

# TFIDF
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Logistic Regression
lr_tfidf = LogisticRegression(max_iter=1000)
lr_tfidf.fit(X_tfidf_train, y_train)
y_pred_tfidf_lr = lr_tfidf.predict(X_tfidf_test)
print("TFIDF - Logistic Regression")
print(classification_report(y_test, y_pred_tfidf_lr))
# Random Forest
rf_tfidf = RandomForestClassifier(n_estimators=100, random_state=1)
rf_tfidf.fit(X_tfidf_train, y_train)
y_pred_tfidf_rf = rf_tfidf.predict(X_tfidf_test)
print("TFIDF - Random Forest")
print(classification_report(y_test, y_pred_tfidf_rf))

# TRANSFORMERS
# Logistic Regression
lr_transf = LogisticRegression(max_iter=1000)
lr_transf.fit(X_transf_train, y_train)
y_pred_transf_lr = lr_transf.predict(X_transf_test)
print("Transformers - Logistic Regression")
print(classification_report(y_test, y_pred_transf_lr))
# Random Forest
rf_transf = RandomForestClassifier(n_estimators=100, random_state=1)
rf_transf.fit(X_transf_train, y_train)
y_pred_transf_rf = rf_transf.predict(X_transf_test)
print("Transformers - Random Forest")
print(classification_report(y_test, y_pred_transf_rf))